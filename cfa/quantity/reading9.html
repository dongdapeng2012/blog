<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>CFA Textbook</title>
</head>
<body>
	<h1>CFA Textbook</h1>
	--------------------
	<h1 id="session2">Study Session 2 Quantitative Methods : Basic
		Concepts</h1>
	-------------------- --------------------
	<h2 id="reading9">Reading 9 Probability Concepts</h2>
	--------------------
	<h3>Summary</h3>
	<ul>
		<br />
		<li>A random variable is a quantity whose outcome is uncertain.</li>
		<br />
		<li>Probability is a number between 0 and 1 that describes the
			chance that a stated event will occur.</li>
		<br />
		<li>An event is a specified set of outcomes of a random variable.</li>
		<br />
		<li>Mutually exclusive events can occur only one at a time.
			Exhaustive events cover or contain all possible outcomes.</li>
		<br />
		<li>The two defining properties of a probability are, first, that
			0 ≤ P(E) ≤ 1 (where P(E) denotes the probability of an event E), and
			second, that the sum of the probabilities of any set of mutually
			exclusive and exhaustive events equals 1.</li>
		<br />
		<li>A probability estimated from data as a relative frequency of
			occurrence is an empirical probability. A probability drawing on
			personal or subjective judgment is a subjective probability. A
			probability obtained based on logical analysis is an a priori
			probability.</li>
		<br />
		<li>A probability of an event E, P(E), can be stated as odds for
			E = P(E)/[1 − P(E)] or odds against E = [1 − P(E)]/P(E).</li>
		<br />
		<li>Probabilities that are inconsistent create profit
			opportunities, according to the Dutch Book Theorem.</li>
		<br />
		<li>A probability of an event not conditioned on another event is
			an unconditional probability. The unconditional probability of an
			event A is denoted P(A). Unconditional probabilities are also called
			marginal probabilities.</li>
		<br />
		<li>A probability of an event given (conditioned on) another
			event is a conditional probability. The probability of an event A
			given an event B is denoted P(A | B).</li>
		<br />
		<li>The probability of both A and B occurring is the joint
			probability of A and B, denoted P(AB).</li>
		<br />
		<li>P(A | B) = P(AB)/P(B), P(B) ≠ 0.</li>
		<br />
		<li>The multiplication rule for probabilities is P(AB) = P(A |
			B)P(B).</li>
		<br />
		<li>The probability that A or B occurs, or both occur, is denoted
			by P(A or B).</li>
		<br />
		<li>The addition rule for probabilities is P(A or B) = P(A) +
			P(B) − P(AB).</li>
		<br />
		<li>When events are independent, the occurrence of one event does
			not affect the probability of occurrence of the other event.
			Otherwise, the events are dependent.</li>
		<br />
		<li>The multiplication rule for independent events states that if
			A and B are independent events, P(AB) = P(A)P(B). The rule
			generalizes in similar fashion to more than two events.</li>
		<br />
		<li>According to the total probability rule, if S1, S2, …, Sn are
			mutually exclusive and exhaustive scenarios or events, then P(A) =
			P(A | S1)P(S1) + P(A | S2)P(S2) + … + P(A | Sn)P(Sn).</li>
		<br />
		<li>The expected value of a random variable is a
			probability-weighted average of the possible outcomes of the random
			variable. For a random variable X, the expected value of X is denoted
			E(X).</li>
		<br />
		<li>The total probability rule for expected value states that
			E(X) = E(X | S1)P(S1) + E(X | S2)P(S2) + … + E(X | Sn)P(Sn), where
			S1, S2, …, Sn are mutually exclusive and exhaustive scenarios or
			events.</li>
		<br />
		<li>The variance of a random variable is the expected value (the
			probability-weighted average) of squared deviations from the random
			variable’s expected value E(X): σ2(X) = E{[X − E(X)]2}, where σ2(X)
			stands for the variance of X.</li>
		<br />
		<li>Variance is a measure of dispersion about the mean.
			Increasing variance indicates increasing dispersion. Variance is
			measured in squared units of the original variable.</li>
		<br />
		<li>Standard deviation is the positive square root of variance.
			Standard deviation measures dispersion (as does variance), but it is
			measured in the same units as the variable.</li>
		<br />
		<li>Covariance is a measure of the co-movement between random
			variables.</li>
		<br />
		<li>The covariance between two random variables Ri and Rj is the
			expected value of the cross-product of the deviations of the two
			random variables from their respective means: Cov(Ri,Rj) = E{[Ri −
			E(Ri)][Rj − E(Rj)]}. The covariance of a random variable with itself
			is its own variance.</li>
		<br />
		<li>Correlation is a number between −1 and +1 that measures the
			co-movement (linear association) between two random variables:
			ρ(Ri,Rj) = Cov(Ri,Rj)/[σ(Ri) σ(Rj)].</li>
		<br />
		<li>To calculate the variance of return on a portfolio of n
			assets, the inputs needed are the n expected returns on the
			individual assets, n variances of return on the individual assets,
			and n(n − 1)/2 distinct covariances.</li>
		<br />
		<li>Portfolio variance of return is σ<sup>2</sup>(R<sup>p</sup>)
			= ∑∑ w<sub>i</sub>w<sub>j</sub>Cov(R<sub>i</sub>,R<sub>j</sub>) .
		</li>
		<br />
		<li>The calculation of covariance in a forward-looking sense
			requires the specification of a joint probability function, which
			gives the probability of joint occurrences of values of the two
			random variables.</li>
		<br />
		<li>When two random variables are independent, the joint
			probability function is the product of the individual probability
			functions of the random variables.</li>
		<br />
		<li>Bayes’ formula is a method for updating probabilities based
			on new information.</li>
		<br />
		<li>Bayes’ formula is expressed as follows: Updated probability
			of event given the new information = [(Probability of the new
			information given event)/(Unconditional probability of the new
			information)] × Prior probability of event.</li>
		<br />
		<li>The multiplication rule of counting says, for example, that
			if the first step in a process can be done in 10 ways, the second
			step, given the first, can be done in 5 ways, and the third step,
			given the first two, can be done in 7 ways, then the steps can be
			carried out in (10)(5)(7) = 350 ways.</li>
		<br />
		<li>The number of ways to assign every member of a group of size
			n to n slots is n! = n (n − 1) (n − 2)(n − 3) … 1. (By convention, 0!
			= 1.)</li>
		<br />
		<li>The number of ways that n objects can be labeled with k
			different labels, with n1 of the first type, n2 of the second type,
			and so on, with n1 + n2 + … + nk = n, is given by n!/(n1!n2! … nk!).
			This expression is the multinomial formula.</li>
		<br />
		<li>combination formula.nCr = n!/(n−r)!r!</li>
		<br />
		<li>nPr = n!/(n−r)!</li>
	</ul>
	<br />
	<h3>中文手册整理</h3>
	<ul>
		<br />
		<li>随机事件，简称事件
			<ul>
				<br />
				<li>互斥事件（mutually exclusive），P(AB) = P(A|B) = P(B|A) = 0</li>
				<br />
				<li>遍历事件（exhaustive events）</li>
				<br />
				<li>所有互斥遍历事件的概率之和等于1</li>
			</ul>
		</li>
		<br />
		<li>分类
			<ul>
				<br />
				<li>客观概率（objective probability）
					<ul>
						<br />
						<li>经验概率（empirical probability）</li>
						<br />
						<li>先验概率（priori probability）</li>
					</ul>
				</li>
				<br />
				<li>主观概率（subjective probability）</li>
			</ul>
		</li>
		<br />
		<li>赔率
			<ul>
				<br />
				<li>odds for E = P(E)/[1-P(E)]，E的胜率</li>
				<br />
				<li>odds against E = [1-P(E)]/P(E)，E的赔率</li>
			</ul>
		</li>
		<br />
		<li>无条件概率（marginal probability） vs 条件概率</li>
		<br />
		<li>联合概率（joint probability），P(AB) = P(A|B)P(B) = P(B|A)P(A)</li>
		<br />
		<li>乘法法则，P(AB) = P(A|B)P(B) = P(B|A)P(A)</li>
		<br />
		<li>加法法则，P(A || B) = P(A) + P (B) - P(AB)</li>
		<br />
		<li>独立事件，P(A|B) = P(A)，或P(AB) = P(A)P(B)</li>
		<br />
		<li>全概率公式 P(A) = 所有情况下A的概率之和</li>
		<br />
		<li>协方差（covariance），Cov(X,Y) = E[(X-E(X))(Y-E(Y))]
			<ul>
				<br />
				<li>两个随机变量变化的同向性</li>
				<br />
				<li>自己和自己的协方差等于方差</li>
				<br />
				<li>协方差的取值范围是负无穷到正无穷</li>
				<br />
				<li>计算
					<ol type="1">
						<br />
						<li>每个变量的期望</li>
						<br />
						<li>各种情况下每个变量对期望的偏差的乘积</li>
						<br />
						<li>所有情况下乘积的期望</li>
					</ol>
				</li>
			</ul>
		</li>
		<br />
		<li>相关系数（correlation），相当于协方差的标准化<img
			src="./image/correlation.png" width="50%" align="middle" />
			<ul>
				<br />
				<li>两个随机变量的线性关系</li>
				<br />
				<li>取值范围[-1,1]</li>
				<br />
				<li>相关系数=0不代表独立</li>
			</ul></li>
		<br />
		<li>贝叶斯公式<img src="./image/Bayes.png" height="50%" width="50%"
			align="middle" /></li>
		<br />
		<li>计数原理
			<ul>
				<br />
				<li>多元二项公式（multinomial formula）</li>
				<br />
				<li>排列（permutation）</li>
				<br />
				<li>组合（combination）</li>
			</ul>
		</li>
	</ul>
	--------------------
	<a href="../quantity/reading10.html">Next Reading</a>
</body>
</html>